{"cells":[{"cell_type":"markdown","source":["# Diving into Delta Lake: Unpacking the Transaction Log\n\n\n\n### Steps to run this notebook\n\nYou can run this notebook in a Databricks environment. Specifically, this notebook has been designed to run in [Databricks Community Edition](http://community.cloud.databricks.com/) as well.\nTo run this notebook, you have to [create a cluster](https://docs.databricks.com/clusters/create.html) with version **Databricks Runtime 6.1 or later** and [attach this notebook](https://docs.databricks.com/notebooks/notebooks-manage.html#attach-a-notebook-to-a-cluster) to that cluster. <br/>&nbsp;\n\n### Source Data for this notebook\n\nThe data used is a modified version of the public data from [Lending Club](https://www.kaggle.com/wendykan/lending-club-loan-data). It includes all funded loans from 2012 to 2017. Each loan includes applicant information provided by the applicant as well as the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. For a full view of the data please view the data dictionary available [here](https://resources.lendingclub.com/LCDataDictionary.xlsx)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9915d168-4455-45ce-947d-898281d2bd80"}}},{"cell_type":"markdown","source":["<img src=\"https://docs.delta.io/latest/_static/delta-lake-logo.png\" width=300/>\n\nAn open-source storage format that brings ACID transactions to Apache Spark™ and big data workloads.\n\n* **Open format**: Stored as Parquet format in blob storage.\n* **ACID Transactions**: Ensures data integrity and read consistency with complex, concurrent data pipelines.\n* **Schema Enforcement and Evolution**: Ensures data cleanliness by blocking writes with unexpected.\n* **Audit History**: History of all the operations that happened in the table.\n* **Time Travel**: Query previous versions of the table by time or version number.\n* **Deletes and upserts**: Supports deleting and upserting into tables with programmatic APIs.\n* **Scalable Metadata management**: Able to handle millions of files are scaling the metadata operations with Spark.\n* **Unified Batch and Streaming Source and Sink**: A table in Delta Lake is both a batch table, as well as a streaming source and sink. Streaming data ingest, batch historic backfill, and interactive queries all just work out of the box."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e15a816f-0beb-4725-80fd-fc77521e8fdf"}}},{"cell_type":"markdown","source":["## Explore data as a Parquet table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e007966-e26b-4119-9d76-9b4688a99d56"}}},{"cell_type":"markdown","source":["#####Download the sampled Lending Club data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be7c3d1f-4011-44ff-ab6a-4752c9ed83d7"}}},{"cell_type":"code","source":["%sh mkdir -p /dbfs/tmp/sais_eu_19_demo/loans/ && wget -O /dbfs/tmp/sais_eu_19_demo/loans/SAISEU19-loan-risks.snappy.parquet  https://pages.databricks.com/rs/094-YMS-629/images/SAISEU19-loan-risks.snappy.parquet && ls -al  /dbfs/tmp/sais_eu_19_demo/loans/ "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16c87dd7-868d-484c-97d4-fdb2d5fd28b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">--2021-06-01 04:16:40--  https://pages.databricks.com/rs/094-YMS-629/images/SAISEU19-loan-risks.snappy.parquet\nResolving pages.databricks.com (pages.databricks.com)... 104.17.72.206, 104.17.73.206, 104.17.74.206, ...\nConnecting to pages.databricks.com (pages.databricks.com)|104.17.72.206|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 164631 (161K) [text/plain]\nSaving to: ‘/dbfs/tmp/sais_eu_19_demo/loans/SAISEU19-loan-risks.snappy.parquet’\n\n     0K .......... .......... .......... .......... .......... 31% 5.63M 0s\n    50K .......... .......... .......... .......... .......... 62% 6.31M 0s\n   100K .......... .......... .......... .......... .......... 93% 6.96M 0s\n   150K ..........                                            100% 4.64M=0.03s\n\n2021-06-01 04:16:40 (6.11 MB/s) - ‘/dbfs/tmp/sais_eu_19_demo/loans/SAISEU19-loan-risks.snappy.parquet’ saved [164631/164631]\n\ntotal 172\ndrwxr-xr-x 2 root root   4096 Jun  1 04:12 .\ndrwxr-xr-x 3 root root   4096 Jun  1 04:12 ..\n-rw-r--r-- 1 root root 164631 May 11 13:00 SAISEU19-loan-risks.snappy.parquet\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">--2021-06-01 04:16:40--  https://pages.databricks.com/rs/094-YMS-629/images/SAISEU19-loan-risks.snappy.parquet\nResolving pages.databricks.com (pages.databricks.com)... 104.17.72.206, 104.17.73.206, 104.17.74.206, ...\nConnecting to pages.databricks.com (pages.databricks.com)|104.17.72.206|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 164631 (161K) [text/plain]\nSaving to: ‘/dbfs/tmp/sais_eu_19_demo/loans/SAISEU19-loan-risks.snappy.parquet’\n\n     0K .......... .......... .......... .......... .......... 31% 5.63M 0s\n    50K .......... .......... .......... .......... .......... 62% 6.31M 0s\n   100K .......... .......... .......... .......... .......... 93% 6.96M 0s\n   150K ..........                                            100% 4.64M=0.03s\n\n2021-06-01 04:16:40 (6.11 MB/s) - ‘/dbfs/tmp/sais_eu_19_demo/loans/SAISEU19-loan-risks.snappy.parquet’ saved [164631/164631]\n\ntotal 172\ndrwxr-xr-x 2 root root   4096 Jun  1 04:12 .\ndrwxr-xr-x 3 root root   4096 Jun  1 04:12 ..\n-rw-r--r-- 1 root root 164631 May 11 13:00 SAISEU19-loan-risks.snappy.parquet\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Setup and configuration**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d055371-eb51-420b-b86a-048d82f9cb0d"}}},{"cell_type":"code","source":["import os, shutil\n\n# Configurations necessary for running of Databricks Community Edition\nspark.sql(\"set spark.sql.shuffle.partitions = 1\")\nspark.sql(\"set spark.databricks.delta.snapshotPartitions = 1\")\n\ndemo_path = \"/sais_eu_19_demo/\"\n\nif os.path.exists(\"/dbfs\" + demo_path):\n  print(\"Deleting path \" + demo_path)\n  shutil.rmtree(\"/dbfs\" + demo_path)\n  print(\"Deleted \" + demo_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9e4e8c6-9592-4627-8f48-b0068f315a9f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#####Create the parquet table \"loans_parquet\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a53e9fcd-38bf-4c28-acb8-a6fdb7f90881"}}},{"cell_type":"code","source":["%sh cp    /dbfs/tmp/sais_eu_19_demo/loans/SAISEU19-loan-risks.snappy.parquet /dbfs/tmp/sais_eu_19_demo/loans_new/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c3fec76-03a7-4630-bcc8-154c1bf89412"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">cp: cannot create regular file &#39;/dbfs/tmp/sais_eu_19_demo/loans_new/&#39;: Not a directory\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">cp: cannot create regular file &#39;/dbfs/tmp/sais_eu_19_demo/loans_new/&#39;: Not a directory\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import os\nimport shutil\nfrom pyspark.sql.functions import * \n\nparquet_path = \"/sais_eu_19_demo/loans_parquet\"\n\n# Delete a new parquet table with the parquet file\nif os.path.exists(\"/dbfs\" + parquet_path):\n  print(\"Deleting path \" + parquet_path)\n  shutil.rmtree(\"/dbfs\" + parquet_path)\n  \n# Create a new parquet table with the parquet file\nspark.read.format(\"parquet\").load(\"/tmp/sais_eu_19_demo/loans\") \\\n  .write.format(\"parquet\").save(parquet_path)\nprint(\"Created a Parquet table at \" + parquet_path)\n\n# Create a view on the table called loans_parquet\nspark.read.format(\"parquet\").load(parquet_path).createOrReplaceTempView(\"loans_parquet\")\nprint(\"Defined view 'loans_parquet'\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61c0c221-7c48-49ea-8fc6-8af23eda8665"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3637542605724024&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> <span class=\"ansi-red-fg\"># Create a new parquet table with the parquet file</span>\n<span class=\"ansi-green-fg\">---&gt; 13</span><span class=\"ansi-red-fg\"> </span>spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;parquet&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/tmp/sais_eu_19_demo/loans&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span>   <span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;parquet&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>parquet_path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Created a Parquet table at &#34;</span> <span class=\"ansi-blue-fg\">+</span> parquet_path<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(self, path, format, schema, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    202</span>         self<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">**</span>options<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    203</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 204</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    205</span>         <span class=\"ansi-green-fg\">elif</span> path <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    206</span>             <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">!=</span> list<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    114</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 116</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    117</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: Unable to infer schema for Parquet. It must be specified manually.</div>","errorSummary":"<span class=\"ansi-red-fg\">AnalysisException</span>: Unable to infer schema for Parquet. It must be specified manually.","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3637542605724024&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> <span class=\"ansi-red-fg\"># Create a new parquet table with the parquet file</span>\n<span class=\"ansi-green-fg\">---&gt; 13</span><span class=\"ansi-red-fg\"> </span>spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;parquet&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/tmp/sais_eu_19_demo/loans&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span>   <span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;parquet&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>parquet_path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Created a Parquet table at &#34;</span> <span class=\"ansi-blue-fg\">+</span> parquet_path<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(self, path, format, schema, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    202</span>         self<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">**</span>options<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    203</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 204</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    205</span>         <span class=\"ansi-green-fg\">elif</span> path <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    206</span>             <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">!=</span> list<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    114</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 116</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    117</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: Unable to infer schema for Parquet. It must be specified manually.</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#####Let's explore this parquet table.\n\n*Schema of the table*\n- load_id - unique id for each loan\n- funded_amnt - principal amount of the loan funded to the loanee\n- paid_amnt - amount from the principle that has been paid back (ignoring interests)\n- addr_state - state where this loan was funded"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e46fb21-9f5a-4eff-b369-24396976a2bb"}}},{"cell_type":"code","source":["spark.sql(\"select * from loans_parquet\").show(20)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ec43efc-9600-477b-aa0b-395644224c7f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**How many records does it have?**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a392c294-03e1-428c-9765-367d52387fc5"}}},{"cell_type":"code","source":["spark.sql(\"select count(*) from loans_parquet\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfd33228-665b-4566-91e8-054b67af79bd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.notebook.exit(\"stop\") # Stop the notebook before the streaming cell, in case of a \"run all\" "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"784324b4-cf5a-4df1-880e-4f5e959f48c8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Let's start appending some new data to it using Structured Streaming.**\n\nWe will generate a stream of data from with randomly generated loan ids and amounts. \nIn addition, we are going to define a few more useful utility functions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5427bf7-6d10-42c7-b0bf-b48dd549e0cc"}}},{"cell_type":"code","source":["import random\nimport os\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n\ndef random_checkpoint_dir(): \n  return \"/sais_eu_19_demo/chkpt/%s\" % str(random.randint(0, 10000))\n\n# User-defined function to generate random state\n\nstates = [\"CA\", \"TX\", \"NY\", \"WA\"]\n\n@udf(returnType=StringType())\ndef random_state():\n  return str(random.choice(states))\n\n# Function to start a streaming query with a stream of randomly generated load data and append to the parquet table\ndef generate_and_append_data_stream(table_format, table_path):\n  \n  stream_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load() \\\n    .withColumn(\"loan_id\", 10000 + col(\"value\")) \\\n    .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \\\n    .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \\\n    .withColumn(\"addr_state\", random_state()) \\\n\n  query = stream_data.writeStream \\\n    .format(table_format) \\\n    .option(\"checkpointLocation\", random_checkpoint_dir()) \\\n    .trigger(processingTime = \"10 seconds\") \\\n    .start(table_path)\n\n  return query\n\n# Function to stop all streaming queries \ndef stop_all_streams():\n  # Stop all the streams\n  print(\"Stopping all streams\")\n  for s in spark.streams.active:\n    s.stop()\n  print(\"Stopped all streams\")\n  print(\"Deleting checkpoints\")  \n  dbutils.fs.rm(\"/sais_eu_19_demo/chkpt/\", True)\n  print(\"Deleted checkpoints\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48839967-4eca-490b-81eb-abb23d53ed64"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Let's start a new stream to append data to the Parquet table**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5f0c22f-9cac-4902-b108-149088faaa90"}}},{"cell_type":"code","source":["stream_query = generate_and_append_data_stream(\n    table_format = \"parquet\", \n    table_path = parquet_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"123c0d83-dbf0-4baf-9840-9b67e1df2599"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Let's see if the data is being added to the table or not**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a613b9d-16bf-4fd4-8672-04f2bfd5ad67"}}},{"cell_type":"code","source":["spark.read.format(\"parquet\").load(parquet_path).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33b3f18e-c094-48d4-8890-e5d2bdf306a2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["** What happens when we try to add a second stream?**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"311f203c-7031-477b-b14e-a94e1bde6e44"}}},{"cell_type":"code","source":["stream_query2 = generate_and_append_data_stream(\n    table_format = \"parquet\", \n    table_path = parquet_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7650150d-7fa4-4582-9804-da628b7fa48a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Where did our existing 14705 rows go? Let's see the data once again**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98ad4613-e31a-4f17-88d4-0278be5f60d7"}}},{"cell_type":"code","source":["spark.read.format(\"parquet\").load(parquet_path).show() # wrong schema!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1d81452-f5d9-4e75-af32-4a480ab49447"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Where did the two new columns `timestamp` and `value` come from? What happened here!**\n\nWhat really happened is that when the streaming query started adding new data to the Parquet table, it did not properly account for the existing data in the table. Furthermore, the new data files that written out accidentally had two extra columns in the schema. Hence, when reading the table, the 2 different schema from different files were merged together, thus unexpectedly modifying the schema of the table.\n\n\nBefore we move on, **if you are running on Databricks Community Edition, definitely stop the streaming queries.** \n\nYou free account in Databricks Community Edition has quota limits on the number of files and we do not want to hit that quote limit by running the streaming queries for too long."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9130b7df-4d74-4b2f-a92f-a3369c2bc10a"}}},{"cell_type":"code","source":["stop_all_streams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af5dcae0-c5ad-46e1-9213-e84a10af35ff"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Batch + stream processing and schema enforcement with Delta Lake\nLet's understand Delta Lake solves these particular problems (among many others). We will start by creating a Delta table from the original data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a716180a-a8d2-4afc-804a-da018e9edeac"}}},{"cell_type":"code","source":["# Configure Delta Lake Silver Path\ndelta_path = \"/sais_eu_19_demo/loans_delta\"\n\n# Configurations necessary for running of Databricks Community Edition\nspark.sql(\"set spark.sql.shuffle.partitions = 1\")\nspark.sql(\"set spark.databricks.delta.snapshotPartitions = 1\")\n\n# Remove folder if it exists\nprint(\"Deleting directory \" + delta_path)\ndbutils.fs.rm(delta_path, recurse=True)\n\n# Create the Delta table with the same loans data\nspark.read.format(\"parquet\").load(\"/tmp/sais_eu_19_demo/loans/SAISEU19-loan-risks.snappy.parquet\") \\\n  .write.format(\"delta\").save(delta_path)\nprint(\"Created a Delta table at \" + delta_path)\n\nspark.read.format(\"delta\").load(delta_path).createOrReplaceTempView(\"loans_delta\")\nprint(\"Defined view 'loans_delta'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b39019d-a0e7-4c54-b7eb-b0aedb49f9f5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Let's see the data once again.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98253e1b-3cf6-493e-bf18-1d4e16e41ae1"}}},{"cell_type":"code","source":["spark.sql(\"select count(*) from loans_delta\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4202032b-e812-41d0-8d4c-8a78193c6c5d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"select * from loans_delta\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"014822f2-471a-44e1-9661-df55fe764e9c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Schema Enforcement"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b20c1549-45ec-4708-bacf-8e92627c704a"}}},{"cell_type":"markdown","source":["**Let's run a streaming count(*) on the table so that the count updates automatically**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"879ef24b-94ed-458c-a096-88b6a753a7f1"}}},{"cell_type":"code","source":["spark.readStream.format(\"delta\").load(delta_path).createOrReplaceTempView(\"loans_delta_stream\")\ndisplay(spark.sql(\"select count(*) from loans_delta_stream\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f53c95bb-cb72-4f1b-a2ab-f23f71c89a7f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Now let's try writing the streaming appends once again**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8552f3db-c151-43f5-804f-e2bcc5d509bc"}}},{"cell_type":"code","source":["stream_query_2 = generate_and_append_data_stream(table_format = \"delta\", table_path = delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db569e17-46f4-40fe-b1ed-2585bf34f5d4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The writes were blocked because the schema of the new data did not match the schema of table (see the exception details). See more information about how it works [here](https://databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html).\n\n**Now, let's fix the streaming query by selecting the columns we want to write.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4154815d-70d3-4961-940f-bbe4a444f7ba"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Generate a stream of randomly generated load data and append to the parquet table\ndef generate_and_append_data_stream_fixed(table_format, table_path):\n    \n  stream_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 50).load() \\\n    .withColumn(\"loan_id\", 10000 + col(\"value\")) \\\n    .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \\\n    .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \\\n    .withColumn(\"addr_state\", random_state()) \\\n    .select(\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\")   # *********** FIXED THE SCHEMA OF THE GENERATED DATA *************\n\n  query = stream_data.writeStream \\\n    .format(table_format) \\\n    .option(\"checkpointLocation\", random_checkpoint_dir()) \\\n    .trigger(processingTime=\"10 seconds\") \\\n    .start(table_path)\n\n  return query\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42fa92e8-bd55-402d-bfc3-c326ad7212ab"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Now we can successfully write to the table. Note the count in the above streaming query increasing as we write to this table.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a9a69d1-fc99-4fe5-bc98-763372a3900d"}}},{"cell_type":"code","source":["stream_query_2 = generate_and_append_data_stream_fixed(table_format = \"delta\", table_path = delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"748442c9-983c-409d-8dc1-6417b5f72a07"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Scroll back up to see the numbers change in the `readStream` as more data is being appended by the `writeStream`.** \n\n**In fact, we can run multiple concurrent streams writing to that table, it will work together.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"265dd1bb-bb19-4072-af8e-71e20e0ad1bd"}}},{"cell_type":"code","source":["stream_query_3 = generate_and_append_data_stream_fixed(table_format = \"delta\", table_path = delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7aadb2a-71e0-4628-8344-448780b15b4c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Just for sanity check, let's query as a batch\n\nNote, you can run a read stream, two write streams, and read in batch - concurrently!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4413676e-3f89-476f-a7ab-3d904331dd33"}}},{"cell_type":"code","source":["display(spark.sql(\"select count(*) from loans_delta\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96721d8b-dac4-4c74-aeab-4f15e5b9fc23"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Again, remember to stop all the streaming queries.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d0bdfaa-ee1b-43db-beb0-a02af06d15fd"}}},{"cell_type":"code","source":["stop_all_streams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11354fe1-8616-47fb-9c28-41ff00a14f52"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Audit Delta Lake Table History\nAll changes to the Delta table are recorded as commits in the table's transaction log. As you write into a Delta table or directory, every operation is automatically versioned. You can use the HISTORY command to view the table's history. For more information, check out the [docs](https://docs.delta.io/latest/delta-utility.html#history)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cac5d00e-58a9-4fe7-9ec4-190a30231377"}}},{"cell_type":"code","source":["%sh \nls -lt /dbfs/sais_eu_19_demo/loans_delta/_delta_log/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c664344d-dc3b-4c34-8a42-c11e3ce644ae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from delta.tables import *\ndelta_path = \"/sais_eu_19_demo/loans_delta\"\n\ndeltaTable = DeltaTable.forPath(spark, delta_path)\ndisplay(deltaTable.history())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"964ba60f-52cf-4237-a242-1631cf65badc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh \nhead /dbfs/sais_eu_19_demo/loans_delta/_delta_log/00000000000000000011.json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05bf6e89-78f7-4672-8d3f-9388e427e115"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["** With Time Travel, you can query previous versions **"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d929f640-c010-429b-81c9-89eb0353168a"}}},{"cell_type":"code","source":["currentVersion = deltaTable.history(1).select(\"version\").collect()[0][0]\n\nv01 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_path).count()\nv11 = spark.read.format(\"delta\").option(\"versionAsOf\", 11).load(delta_path).count()\nvno = spark.read.format(\"delta\").option(\"versionAsOf\", currentVersion).load(delta_path).count()\nprint(\"loans_delta table counts:\\n Initial [%s] \\n Version 11 [%s] \\n Current Version [%s]\" % (v01, v11, vno))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"548d6000-6995-4894-a87a-88e2ded18f18"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["** But, there are a lot of files because of the data versioning.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24ab04ce-fd42-4280-953b-48779b0ebd4a"}}},{"cell_type":"code","source":["%sh \nls -lt /dbfs/sais_eu_19_demo/loans_delta/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6a951c0-cc5f-453e-b528-ab17c75eb7a1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["%md ###  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Compact files\n\nIf you continuously write data to a Delta table, it will over time accumulate a large number of files, especially if you add data in small batches. This can have an adverse effect on the efficiency of table reads, and it can also affect the performance of your file system. Ideally, a large number of small files should be rewritten into a smaller number of larger files on a regular basis. This is known as compaction.\n\nYou can compact a table by repartitioning it to smaller number of files. In addition, you can specify the option `dataChange` to be false indicates that the operation does not change the data, only rearranges the data layout. This would ensure that other concurrent operations are minimally affected due to this compaction operation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f34eae1-171d-4eb3-8bc2-7a2afa9b5fa4"}}},{"cell_type":"code","source":["numFiles = 4\n\n(spark.read\n   .format(\"delta\")\n   .load(delta_path)\n   .repartition(numFiles)\n   .write\n   .option(\"dataChange\", \"false\")\n   .format(\"delta\")\n   .mode(\"overwrite\")\n   .save(delta_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb318ed5-98cf-41e1-bb3d-1c19495b7912"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh \nls -lt /dbfs/sais_eu_19_demo/loans_delta/ | wc -l"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8312ff7c-46a2-44f7-aada-04c6183aad15"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# This is an anti-pattern\nspark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")\ndeltaTable.vacuum(retentionHours = 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c312d8d-6ddf-4941-b694-5622d9eddc6f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh \nls -lt /dbfs/sais_eu_19_demo/loans_delta/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f1547ea-2a5c-41dd-81a2-8da89443b4ac"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Join the community!\n\n\n* [Delta Lake on GitHub](https://github.com/delta-io/delta)\n* [Delta Lake Slack Channel](https://delta-users.slack.com/) ([Registration Link](https://join.slack.com/t/delta-users/shared_invite/enQtNTY1NDg0ODcxOTI1LWJkZGU3ZmQ3MjkzNmY2ZDM0NjNlYjE4MWIzYjg2OWM1OTBmMWIxZTllMjg3ZmJkNjIwZmE1ZTZkMmQ0OTk5ZjA))\n* [Public Mailing List](https://groups.google.com/forum/#!forum/delta-users)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e26575d7-9f88-4491-9598-6417085aac18"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Diving Into Delta Lake -  Unpacking The Transaction Log","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3637542605724015}},"nbformat":4,"nbformat_minor":0}
