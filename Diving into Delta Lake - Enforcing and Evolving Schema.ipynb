{"cells":[{"cell_type":"markdown","source":["# Diving into Delta Lake: Enforcing and Evolving the Schema\n\n\n### Steps to run this notebook\n\nYou can run this notebook in a Databricks environment. Specifically, this notebook has been designed to run in [Databricks Community Edition](http://community.cloud.databricks.com/) as well.\nTo run this notebook, you have to [create a cluster](https://docs.databricks.com/clusters/create.html) with version **Databricks Runtime 6.1 or later** and [attach this notebook](https://docs.databricks.com/notebooks/notebooks-manage.html#attach-a-notebook-to-a-cluster) to that cluster. <br/>&nbsp;\n\n### Source Data for this notebook\n\nThe data used is a modified version of the public data from [Lending Club](https://www.kaggle.com/wendykan/lending-club-loan-data). It includes all funded loans from 2012 to 2017. Each loan includes applicant information provided by the applicant as well as the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. For a full view of the data please view the data dictionary available [here](https://resources.lendingclub.com/LCDataDictionary.xlsx)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"637083e3-d80c-43a1-8f5a-5a1109a82121"}}},{"cell_type":"markdown","source":["<img src=\"https://docs.delta.io/latest/_static/delta-lake-logo.png\" width=300/>\n\nAn open-source storage format that brings ACID transactions to Apache Sparkâ„¢ and big data workloads.\n\n* **Open format**: Stored as Parquet format in blob storage.\n* **ACID Transactions**: Ensures data integrity and read consistency with complex, concurrent data pipelines.\n* **Schema Enforcement and Evolution**: Ensures data cleanliness by blocking writes with unexpected.\n* **Audit History**: History of all the operations that happened in the table.\n* **Time Travel**: Query previous versions of the table by time or version number.\n* **Deletes and upserts**: Supports deleting and upserting into tables with programmatic APIs.\n* **Scalable Metadata management**: Able to handle millions of files are scaling the metadata operations with Spark.\n* **Unified Batch and Streaming Source and Sink**: A table in Delta Lake is both a batch table, as well as a streaming source and sink. Streaming data ingest, batch historic backfill, and interactive queries all just work out of the box."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ec82a37-3c60-4937-8db3-ce984038f25a"}}},{"cell_type":"markdown","source":["## Explore data as a Parquet table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab39e2b5-38aa-48ea-9a7c-323d7137e69b"}}},{"cell_type":"markdown","source":["#####Download the sampled Lending Club data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acaf81fb-0c42-4abc-b629-73c29a7a67fb"}}},{"cell_type":"code","source":["%sh mkdir -p /dbfs/tmp/sais_eu_19_demo/loans/ && wget -O /dbfs/tmp/sais_eu_19_demo/loans/SAISEU19-loan-risks.snappy.parquet  https://pages.databricks.com/rs/094-YMS-629/images/SAISEU19-loan-risks.snappy.parquet && ls -al  /dbfs/tmp/sais_eu_19_demo/loans/ "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf224a5a-cb81-44d4-b162-8b5c894b2786"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Setup and configuration**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b28cd694-8495-473a-92b9-14bfdab5fba1"}}},{"cell_type":"code","source":["import os, shutil\n\n# Configurations necessary for running of Databricks Community Edition\nspark.sql(\"set spark.sql.shuffle.partitions = 1\")\nspark.sql(\"set spark.databricks.delta.snapshotPartitions = 1\")\n\ndemo_path = \"/sais_eu_19_demo/\"\n\nif os.path.exists(\"/dbfs\" + demo_path):\n  print(\"Deleting path \" + demo_path)\n  shutil.rmtree(\"/dbfs\" + demo_path)\n  print(\"Deleted \" + demo_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94f42602-f888-4114-8d15-f32ac9930283"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#####Create the parquet table \"loans_parquet\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd54b3ae-5f76-4d10-a1c8-e087731a9c66"}}},{"cell_type":"code","source":["import os\nimport shutil\nfrom pyspark.sql.functions import * \n\nparquet_path = \"/sais_eu_19_demo/loans_parquet\"\n\n# Delete parquet table if it exsists\nif os.path.exists(\"/dbfs\" + parquet_path):\n  print(\"Deleting path \" + parquet_path)\n  shutil.rmtree(\"/dbfs\" + parquet_path)\n  \n# Create a new parquet table with the parquet file\nspark.read.format(\"parquet\").load(\"/tmp/sais_eu_19_demo/loans/SAISEU19-loan-risks.snappy.parquet\") \\\n  .write.format(\"parquet\").save(parquet_path)\nprint(\"Created a Parquet table at \" + parquet_path)\n\n# Create a view on the table called loans_parquet\nspark.read.format(\"parquet\").load(parquet_path).createOrReplaceTempView(\"loans_parquet\")\nprint(\"Defined view 'loans_parquet'\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08a963e8-84a3-4c5a-81a0-43526b4dc30b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#####Let's explore this parquet table.\n\n*Schema of the table*\n- load_id - unique id for each loan\n- funded_amnt - principal amount of the loan funded to the loanee\n- paid_amnt - amount from the principle that has been paid back (ignoring interests)\n- addr_state - state where this loan was funded"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce271715-0a95-4fb4-931a-386ba8ed7df7"}}},{"cell_type":"code","source":["spark.sql(\"select * from loans_parquet\").show(20)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e959458-dfd5-4a2e-8230-bb922c5c4b43"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**How many records does it have?**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b11df00e-2d30-4270-b773-b39969a4b9ac"}}},{"cell_type":"code","source":["spark.sql(\"select count(*) from loans_parquet\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cca84b73-951b-4d65-98fb-f0b40fd331b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.notebook.exit(\"stop\") # Stop the notebook before the streaming cell, in case of a \"run all\" "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebd1cedf-6f0f-43b2-badf-326589946e41"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Let's start appending some new data to it using Structured Streaming.**\n\nWe will generate a stream of data from with randomly generated loan ids and amounts. \nIn addition, we are going to define a few more useful utility functions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3afc1a6b-1e51-4a42-8c6a-a141aec9eb30"}}},{"cell_type":"code","source":["import random\nimport os\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n\ndef random_checkpoint_dir(): \n  return \"/sais_eu_19_demo/chkpt/%s\" % str(random.randint(0, 10000))\n\n# User-defined function to generate random state\n\nstates = [\"CA\", \"TX\", \"NY\", \"WA\"]\n\n@udf(returnType=StringType())\ndef random_state():\n  return str(random.choice(states))\n\n# Function to start a streaming query with a stream of randomly generated load data and append to the parquet table\ndef generate_and_append_data_stream(table_format, table_path):\n  \n  stream_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load() \\\n    .withColumn(\"loan_id\", 10000 + col(\"value\")) \\\n    .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \\\n    .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \\\n    .withColumn(\"addr_state\", random_state()) \\\n\n  query = stream_data.writeStream \\\n    .format(table_format) \\\n    .option(\"checkpointLocation\", random_checkpoint_dir()) \\\n    .trigger(processingTime = \"10 seconds\") \\\n    .start(table_path)\n\n  return query\n\n# Function to stop all streaming queries \ndef stop_all_streams():\n  # Stop all the streams\n  print(\"Stopping all streams\")\n  for s in spark.streams.active:\n    s.stop()\n  print(\"Stopped all streams\")\n  print(\"Deleting checkpoints\")  \n  dbutils.fs.rm(\"/sais_eu_19_demo/chkpt/\", True)\n  print(\"Deleted checkpoints\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06579b2a-c39d-4a79-be34-3e69f5f01afa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Let's start a new stream to append data to the Parquet table**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1765948-313a-4c4f-a140-5637aebf9d08"}}},{"cell_type":"code","source":["stream_query = generate_and_append_data_stream(\n    table_format = \"parquet\", \n    table_path = parquet_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b86e681-b070-4e42-b795-e1676ce09fe3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Let's see if the data is being added to the table or not**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c31b9912-4bca-42b7-8577-7b40482dc13b"}}},{"cell_type":"code","source":["spark.read.format(\"parquet\").load(parquet_path).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"793d45e0-c6e5-4629-98bf-7762ad173780"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Where did our existing 14705 rows go? Let's see the data once again**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35b236b0-1f40-44c6-b575-b3930ebdb867"}}},{"cell_type":"code","source":["spark.read.format(\"parquet\").load(parquet_path).show() # wrong schema!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e51c0a5-d65f-4815-ba41-627a88f22180"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Where did the two new columns `timestamp` and `value` come from? What happened here!**\n\nWhat really happened is that when the streaming query started adding new data to the Parquet table, it did not properly account for the existing data in the table. Furthermore, the new data files that written out accidentally had two extra columns in the schema. Hence, when reading the table, the 2 different schema from different files were merged together, thus unexpectedly modifying the schema of the table.\n\n\nBefore we move on, **if you are running on Databricks Community Edition, definitely stop the streaming queries.** \n\nYou free account in Databricks Community Edition has quota limits on the number of files and we do not want to hit that quote limit by running the streaming queries for too long."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2faaeccb-46ff-4782-a1e2-9e4c77d23ac8"}}},{"cell_type":"code","source":["stop_all_streams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21101d84-5ad3-436e-ae61-4d33f925fba5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Batch + stream processing and schema enforcement with Delta Lake\nLet's understand how Delta Lake solves these particular problems (among many others). We will start by creating a Delta table from the original data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdb046bf-c298-4270-8211-ef563b24997d"}}},{"cell_type":"code","source":["# Configure Delta Lake Silver Path\ndelta_path = \"/sais_eu_19_demo/loans_delta\"\n\n# Configurations necessary for running of Databricks Community Edition\nspark.sql(\"set spark.sql.shuffle.partitions = 1\")\nspark.sql(\"set spark.databricks.delta.snapshotPartitions = 1\")\n\n# Remove folder if it exists\nprint(\"Deleting directory \" + delta_path)\ndbutils.fs.rm(delta_path, recurse=True)\n\n# Create the Delta table with the same loans data\nspark.read.format(\"parquet\").load(\"/tmp/sais_eu_19_demo/loans/SAISEU19-loan-risks.snappy.parquet\") \\\n  .write.format(\"delta\").save(delta_path)\nprint(\"Created a Delta table at \" + delta_path)\n\nspark.read.format(\"delta\").load(delta_path).createOrReplaceTempView(\"loans_delta\")\nprint(\"Defined view 'loans_delta'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82bddb29-bc77-4a93-92ff-8e119a05141a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Let's see the data once again.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22d2e112-3122-469c-939b-bde8ee803ad6"}}},{"cell_type":"code","source":["spark.sql(\"select count(*) from loans_delta\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f34e3e49-21de-46ec-b2c8-71d32eb2c9a5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"select * from loans_delta\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dea5a83a-70fd-4ff9-8d29-0307736a07b2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Review the Schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9af597dd-e029-40fa-a21e-c5e7473a04ea"}}},{"cell_type":"code","source":["%sh \nls -lt /dbfs/sais_eu_19_demo/loans_delta/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"419bf806-b608-425a-badb-5ee62216d554"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh \nls -lt /dbfs/sais_eu_19_demo/loans_delta/_delta_log"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e37ba9b9-508d-4cda-b7e3-fca3044142ec"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh \nhead /dbfs/sais_eu_19_demo/loans_delta/_delta_log/00000000000000000000.json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de4ab782-c499-472e-accb-ad4f9e4872b3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["j0 = spark.read.json(\"/sais_eu_19_demo/loans_delta/_delta_log/00000000000000000000.json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79a484b5-7cd6-47e9-9ea6-2c992bb0031d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Commit Information"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ac9ab15-09d4-4845-aca4-8487f613a0dd"}}},{"cell_type":"code","source":["# Commit Information\ndisplay(j0.select(\"commitInfo\").where(\"commitInfo is not null\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15c9ef76-472f-4ae3-bc9e-2b9f5eaab61f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Add"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f69a485-d8db-4625-863d-30b43be2e45a"}}},{"cell_type":"code","source":["# Add Information\ndisplay(j0.select(\"add\").where(\"add is not null\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c1e9ef2-9644-4f16-b423-a31a78e869eb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Metadata\nNote the `schemaString`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b836cfb4-6438-4150-a764-30ebb141fe71"}}},{"cell_type":"code","source":["# Add Information\ndisplay(j0.select(\"metadata\").where(\"metadata is not null\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fd71ac9-247b-4fc5-add0-dc8141bfaa08"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Schema Enforcement"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"094cf4d1-c938-4115-b615-d4213518ea64"}}},{"cell_type":"markdown","source":["**Let's run a streaming count(*) on the table so that the count updates automatically**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46b4c4d5-ca22-44c1-af3c-0f166b1cb001"}}},{"cell_type":"code","source":["spark.readStream.format(\"delta\").load(delta_path).createOrReplaceTempView(\"loans_delta_stream\")\ndisplay(spark.sql(\"select count(*) from loans_delta_stream\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12f55dfd-3b14-43b9-a638-e4af4524d42c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Now let's try writing the streaming appends once again**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12dd2a5c-7ede-4514-9eaf-d2200beefef1"}}},{"cell_type":"code","source":["stream_query_2 = generate_and_append_data_stream(table_format = \"delta\", table_path = delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b4b37f9-7884-4714-9d13-70277f75fb27"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The writes were blocked because the schema of the new data did not match the schema of table (see the exception details). See more information about how it works [here](https://databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html).\n\n**Now, let's fix the streaming query by selecting the columns we want to write.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99626061-d1d2-4884-9e5e-599abceddc54"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Generate a stream of randomly generated load data and append to the parquet table\ndef generate_and_append_data_stream_fixed(table_format, table_path):\n    \n  stream_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 50).load() \\\n    .withColumn(\"loan_id\", 10000 + col(\"value\")) \\\n    .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \\\n    .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \\\n    .withColumn(\"addr_state\", random_state()) \\\n    .select(\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\")   # *********** FIXED THE SCHEMA OF THE GENERATED DATA *************\n\n  query = stream_data.writeStream \\\n    .format(table_format) \\\n    .option(\"checkpointLocation\", random_checkpoint_dir()) \\\n    .trigger(processingTime=\"10 seconds\") \\\n    .start(table_path)\n\n  return query\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e6d5ae2-4fb1-482b-9133-3dde990b69ce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Now we can successfully write to the table. Note the count in the above streaming query increasing as we write to this table.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf442591-b8ea-4b79-87a7-809022c21e45"}}},{"cell_type":"code","source":["stream_query_2 = generate_and_append_data_stream_fixed(table_format = \"delta\", table_path = delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dbe593e-5fb3-4817-ae0b-77cd8659a839"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Scroll back up to see the numbers change in the `readStream` as more data is being appended by the `writeStream`.** \n\n**In fact, we can run multiple concurrent streams writing to that table, it will work together.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"739a981b-9a01-4ac4-ab77-f8428034f663"}}},{"cell_type":"markdown","source":["Just for sanity check, let's query as a batch\n\nNote, you can run a read stream, two write streams, and read in batch - concurrently!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5365fc73-903e-441f-acc2-30547ab562d4"}}},{"cell_type":"code","source":["display(spark.sql(\"select count(*) from loans_delta\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7c3dc2b-a020-41fc-ba99-8415d8b002dc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Again, remember to stop all the streaming queries.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c643d73d-f575-48d7-a312-c9dfad13ef07"}}},{"cell_type":"code","source":["stop_all_streams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7283d06-d9b9-42e3-8835-4c3a48ed1992"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh \nls -lt /dbfs/sais_eu_19_demo/loans_delta/_delta_log/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68b2f7cd-a9ae-4614-b2bf-bf80449ffdba"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from delta.tables import *\ndelta_path = \"/sais_eu_19_demo/loans_delta\"\ndeltaTable = DeltaTable.forPath(spark, delta_path)\n\n# remember the last commit before schema change\nc_before = deltaTable.history(1).select(\"version\").collect()[0][0]\nprint(c_before)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f494b1d-072b-4aba-9894-e1e419ff3d38"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Schema Evolution"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f59df1a-2ee9-4507-bed0-785939b14e28"}}},{"cell_type":"markdown","source":["d **Let's run a streaming count(*) on the table so that the count updates automatically**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35f6c01e-ce0e-4a7e-9498-e56e04dda494"}}},{"cell_type":"code","source":["spark.readStream.format(\"delta\").load(delta_path).createOrReplaceTempView(\"loans_delta_stream\")\ndisplay(spark.sql(\"select count(*) from loans_delta_stream\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1f3e4c1-ece2-43ef-8343-0b7bd1768599"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Now let's try writing the streaming appends once again**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e01de8b-9403-4728-bc61-458655428243"}}},{"cell_type":"code","source":["stream_query_2 = generate_and_append_data_stream(table_format = \"delta\", table_path = delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a41f9f3-e8f7-48e4-99e8-74a9becd4662"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Generate a stream of randomly generated load data and append to the table (with mergeSchema option)\ndef generate_and_append_data_stream_mergeSchema(table_format, table_path):\n  \n  stream_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load() \\\n    .withColumn(\"loan_id\", 10000 + col(\"value\")) \\\n    .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \\\n    .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \\\n    .withColumn(\"addr_state\", random_state())\n\n  query = stream_data.writeStream \\\n    .format(table_format) \\\n    .option(\"checkpointLocation\", random_checkpoint_dir()) \\\n    .option(\"mergeSchema\", \"true\") \\\n    .trigger(processingTime = \"10 seconds\") \\\n    .start(table_path)\n\n  return query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdda2f8f-7f75-4ec1-a326-e7bdc585f28d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stream_query_2 = generate_and_append_data_stream_mergeSchema(table_format = \"delta\", table_path = delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2ed4541-fea6-4e17-b552-7e498e8a1f07"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.readStream.format(\"delta\").load(delta_path).createOrReplaceTempView(\"loans_delta_stream\")\ndisplay(spark.sql(\"select count(*) from loans_delta_stream\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea7814fc-5ddc-4399-a8d5-6c420ab12882"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"select count(1) from loans_delta\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59d0de91-e5c1-4872-9462-5f4ba5e9dfc1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Again, remember to stop all the streaming queries.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22b6b55f-03fe-4d02-a0f5-448ebf01452a"}}},{"cell_type":"code","source":["stop_all_streams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a27b6e07-0be1-48e0-9f88-62a1dab1f612"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Audit Delta Lake Table History\nAll changes to the Delta table are recorded as commits in the table's transaction log. As you write into a Delta table or directory, every operation is automatically versioned. You can use the HISTORY command to view the table's history. For more information, check out the [docs](https://docs.delta.io/latest/delta-utility.html#history)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d02009b-aa41-4589-a813-f9c3b1b5506c"}}},{"cell_type":"code","source":["%sh \nls -lt /dbfs/sais_eu_19_demo/loans_delta/_delta_log/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c647a29e-defc-4eb3-90b2-679d6d93fee8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from delta.tables import *\ndelta_path = \"/sais_eu_19_demo/loans_delta\"\n\ndeltaTable = DeltaTable.forPath(spark, delta_path)\ndisplay(deltaTable.history())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cdbd01e-41d2-41ad-adf5-cddfd5fd4836"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Review JSON for Schema Changes\n* `commit_before`: Data with old schema\n* `commit_change`: Transaction for schema change\n* `commit_after`: New data with new schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8418ed3-ead9-4893-a658-169f5e364d91"}}},{"cell_type":"code","source":["c_change = c_before + 1\nc_after = c_change + 1\npath_template = \"/sais_eu_19_demo/loans_delta/_delta_log/0000000000000000{commit:04d}.json\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b0f907e-efd6-4f82-ab79-ec5fb2f72dc3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["commit_before = spark.read.json(path_template.format(commit = c_before))\ncommit_change = spark.read.json(path_template.format(commit = c_change))\ncommit_after = spark.read.json(path_template.format(commit = c_after))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc3a6cf2-66b1-4a38-959c-f083c34e6f44"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Add Information"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11ae3a06-dc48-42e8-a1a0-7b009900fb90"}}},{"cell_type":"code","source":["# Data with old schema\ndisplay(commit_before.select(\"add\").where(\"add is not null\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11647e04-3f9b-45ab-a72c-86f3b7c6231f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Transaction for schema change\ndisplay(commit_change.select(\"add\").where(\"add is not null\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e23f340-4917-48f6-9789-a122ca851a26"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Metadata Information"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b0049a3-bdce-4390-ba9e-ec18542f026a"}}},{"cell_type":"code","source":["# Metadata Information\ndisplay(commit_change.select(\"metadata\").where(\"metadata is not null\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cb36a77-ca99-4ef3-9d5f-d782ab7aa5b3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# New data with new schema\ndisplay(commit_after.select(\"add\").where(\"add is not null\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f5aafa3-101f-4564-841d-b9393b93ae3c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Time Travel - Querying historic versions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3589b2f6-7a27-4df9-a8a0-106b2586f1ed"}}},{"cell_type":"code","source":["currentVersion = deltaTable.history(1).select(\"version\").collect()[0][0]\n\nv_init = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_path).count()\nv_change = spark.read.format(\"delta\").option(\"versionAsOf\", c_before).load(delta_path).count()\nv_now = spark.read.format(\"delta\").option(\"versionAsOf\", currentVersion).load(delta_path).count()\nprint(\"loans_delta table counts:\\n Initial [%s]\\n At Schema Change [%s]\\n Current Version [%s]\" % (v_init, v_change, v_now))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ff0747b-ac2c-47b1-b5e1-73d496da98cc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.read.format(\"delta\").option(\"versionAsOf\", c_before).load(delta_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca5bde78-cf49-4f4a-9de1-239041b64c06"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.read.format(\"delta\").option(\"versionAsOf\", c_change).load(delta_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ded8a487-417f-4eb3-9f23-6dbda39f31df"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.read.format(\"delta\").option(\"versionAsOf\", c_after).load(delta_path).where(\"timestamp is not null\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9d20d91-edb5-4ec7-989e-7741048112ed"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Join the community!\n\n\n* [Delta Lake on GitHub](https://github.com/delta-io/delta)\n* [Delta Lake Slack Channel](https://delta-users.slack.com/) ([Registration Link](https://join.slack.com/t/delta-users/shared_invite/enQtNTY1NDg0ODcxOTI1LWJkZGU3ZmQ3MjkzNmY2ZDM0NjNlYjE4MWIzYjg2OWM1OTBmMWIxZTllMjg3ZmJkNjIwZmE1ZTZkMmQ0OTk5ZjA))\n* [Public Mailing List](https://groups.google.com/forum/#!forum/delta-users)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8764652b-4904-4e5b-b020-1495aa9468a1"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Diving into Delta Lake - Enforcing and Evolving Schema","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3637542605724079}},"nbformat":4,"nbformat_minor":0}
